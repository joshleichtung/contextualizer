# Feedback & Metrics System

**Purpose**: Maximize learning and improvement by combining automated metrics, inferred usage patterns, and explicit user feedback with minimal friction.

---

## Three-Tier Feedback System

### Tier 1: Automatic Metrics (Zero Effort)

**What We Capture Automatically**:

#### From MCP Server Logs
```
.contextualizer/mcp.log
- Tool invocation timestamps
- Tool execution duration
- Error rates by tool
- Parameter patterns
- Frequency of use by tool
```

#### From Hook Execution
```
.contextualizer/hook-errors.log
- Context warning frequency
- Warning â†’ action latency (did user clear context?)
- Hook execution time
- False positive warnings
```

#### From Git History
```
git log analysis
- Commit frequency during Contextualizer usage
- Story completion velocity
- Rework frequency (re-commits to same files)
```

#### Inferred Metrics
- **Context management effectiveness**: Warning â†’ clear latency
- **Tool usage patterns**: Which tools used most/least
- **Performance actuals**: vs NFR targets
- **Workflow friction**: Time between tool invocations
- **Feature adoption**: New tools used within 24h of release

**Implementation**: Epic 5, Story 5.6 (Metrics Collection)

---

### Tier 2: One-Click Feedback (Minimal Effort)

**Quick Feedback Commands via Claude**:

#### During Development
```
You: "Log feedback: context warning was helpful"
Claude: âœ… Logged positive feedback for context-monitoring

You: "Log bug: init_project failed on monorepo"
Claude: ğŸ› Created issue #42 for init_project monorepo bug

You: "Log idea: add Python preset"
Claude: ğŸ’¡ Added idea to backlog: Python preset support
```

#### Quick Reaction System
```
After any Contextualizer operation:

Claude: "Did that help? Reply: y/n/skip"
You: "y"
Claude: âœ… Logged positive feedback for [operation]

OR

You: "n - thresholds too sensitive"
Claude: ğŸ“ Logged: context thresholds need adjustment
```

**MCP Tool**: `log_feedback`
```typescript
{
  type: 'bug' | 'friction' | 'idea' | 'positive' | 'negative';
  component: string; // auto-detected or specified
  message: string;
  context?: {
    story: string;
    tool: string;
    timestamp: string;
  };
}
```

**Implementation**: Epic 6, Story 6.6 (Feedback System)

---

### Tier 3: Structured Reflection (Weekly)

**Weekly Review Prompt**:

Every Sunday (or your preferred day), Claude prompts:

```
ğŸ“Š Weekly Contextualizer Review

Based on your usage this week:
- 47 tool invocations (â†‘23% from last week)
- 12 context warnings (85% followed by clear action)
- 3.2s avg tool response time (target: 2s) âš ï¸

Quick questions (optional - skip any):

1. Which feature helped most this week?
   [ Type answer or "skip" ]

2. Biggest friction point?
   [ Type answer or "skip" ]

3. One thing to improve?
   [ Type answer or "skip" ]

Your feedback is saved to docs/feedback/weekly-YYYY-MM-DD.md
```

**Auto-Generated Weekly Report**:
```markdown
# Week of 2025-10-20

## Metrics
- Tool invocations: 47 (â†‘23%)
- Top tools: init_project (15), run_doctor (12), configure_hooks (8)
- Avg response time: 3.2s (âš ï¸ above 2s target)
- Context warnings: 12 (85% action rate)

## Inferred Insights
- âœ… High action rate on warnings suggests they're helpful
- âš ï¸ Performance degradation needs investigation
- ğŸ“ˆ Increased usage of init_project (new projects?)

## User Feedback
- [Your answers to weekly questions]

## Auto-Generated Action Items
- [ ] Investigate performance regression (3.2s vs 2s target)
- [ ] Consider adding [detected pattern] to presets
```

**Implementation**: Epic 6, Story 6.7 (Analytics & Insights)

---

## Easy Feedback Mechanisms

### 1. Natural Language Feedback (Easiest)

**Just talk to Claude**:
```
"The context warning interrupted my flow - can we make it less intrusive?"
â†’ Auto-logged as friction, categorized as UX issue

"init_project was perfect for my React project"
â†’ Auto-logged as positive feedback, tagged to init_project + React preset

"Bug: doctor missed that my hooks weren't executable"
â†’ Created issue, added to backlog, linked to diagnostics engine
```

**Behind the Scenes**:
- Claude detects feedback intent
- Categorizes automatically (bug/friction/idea/positive)
- Links to relevant component
- Creates actionable items

---

### 2. Reaction Emojis (One Character)

**After any Contextualizer operation**:
```
Claude: "Project initialized with web-fullstack preset âœ…
         React as? ğŸ‘ ğŸ‘ ğŸ’¡ ğŸ› (or skip)"

You: "ğŸ‘"
â†’ Positive feedback logged

You: "ğŸ’¡ add Vue preset"
â†’ Idea logged to backlog
```

**Emoji Meanings**:
- ğŸ‘ = Positive feedback
- ğŸ‘ = Negative feedback / friction
- ğŸ’¡ = Idea / feature request
- ğŸ› = Bug report
- â­ï¸ = Skip

---

### 3. Inline Feedback (During Usage)

**Contextual Prompts**:
```
After context warning:
"Was this warning helpful? (y/n or feedback)"

After tool execution:
"init_project completed in 2.8s. Performance okay? (y/n)"

After doctor autofix:
"Fixed 3 issues. All correct? (y/n or details)"
```

---

### 4. Screenshot/Recording Feedback

**For Complex Issues**:
```
You: "Log visual bug" [attach screenshot]
Claude: ğŸ“¸ Logged visual issue with screenshot
        Created: docs/feedback/bugs/2025-10-20-visual-001.md

OR

You: "Record workflow issue"
Claude: ğŸ¥ Start screen recording, then say "stop recording"
        [you demonstrate issue]
You: "Stop recording"
Claude: Saved recording + created issue #43
```

---

## Feedback Categories & Routing

### Auto-Categorization

**Bug Reports** â†’ Create GitHub issue
```yaml
priority: based on frequency/severity
assignee: relevant component owner
labels: [bug, component-name, auto-detected]
linked_to: story/epic if detected
```

**Friction Points** â†’ Add to improvement backlog
```yaml
type: UX | Performance | Workflow
impact: high | medium | low (based on frequency)
effort: estimate based on component
priority_score: impact Ã— frequency / effort
```

**Ideas** â†’ Add to feature backlog
```yaml
phase: 1-5 (based on roadmap alignment)
value: inferred from context
effort: rough estimate
roi_score: value / effort
```

**Positive Feedback** â†’ Validate decisions
```yaml
component: what worked well
pattern: why it worked
preserve: don't break this in future
```

---

## Metrics Dashboard

### Real-Time View (In Claude)

**Quick Status**:
```
You: "Show Contextualizer stats"

Claude: "ğŸ“Š Contextualizer Metrics (Last 7 Days)

Usage:
â”œâ”€ 47 tool invocations (â†‘23%)
â”œâ”€ init_project: 15 (most used)
â”œâ”€ run_doctor: 12
â””â”€ configure_hooks: 8

Performance:
â”œâ”€ Avg response: 3.2s (âš ï¸ target: 2s)
â”œâ”€ Hook execution: 78ms (âœ… target: 100ms)
â””â”€ Doctor run: 4.1s (âœ… target: 5s)

Quality:
â”œâ”€ Context warnings: 12 (85% action rate âœ…)
â”œâ”€ Doctor accuracy: 94% (âœ…)
â””â”€ Error rate: 2% (âœ…)

Feedback:
â”œâ”€ 8 positive
â”œâ”€ 3 friction points
â””â”€ 5 ideas logged"
```

### Weekly Summary (Auto-Generated)

**Delivered Every Sunday**:
```markdown
# Contextualizer Weekly Insights - Week 42

## ğŸ¯ Highlights
- You're using Contextualizer 47 times/week (healthy adoption!)
- Context warnings have 85% action rate (they're working!)
- Performance needs attention (3.2s vs 2s target)

## ğŸ“Š Key Metrics
[Charts and graphs would go here in visual version]

## ğŸ’¡ Recommendations
Based on your usage patterns:
1. Consider creating a custom preset for your workflow
2. Adjust context thresholds (you clear at 75% avg)
3. Performance investigation needed for init_project

## ğŸ¯ Action Items Created
- #45: Investigate init_project performance
- #46: Add custom preset support
- #47: Adjust default thresholds based on usage

## ğŸ™ Quick Feedback
[3 optional questions here]
```

---

## Feedback Storage & Analysis

### File Structure
```
docs/feedback/
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ 2025-W42.json          # Weekly metrics
â”‚   â”œâ”€â”€ tool-usage.json         # Aggregate tool usage
â”‚   â””â”€â”€ performance.json        # Performance tracking
â”œâ”€â”€ user-feedback/
â”‚   â”œâ”€â”€ 2025-10-20-positive.md  # Positive feedback log
â”‚   â”œâ”€â”€ 2025-10-20-friction.md  # Friction points log
â”‚   â””â”€â”€ 2025-10-20-ideas.md     # Ideas and requests
â”œâ”€â”€ bugs/
â”‚   â”œâ”€â”€ 2025-10-20-001.md       # Bug reports
â”‚   â””â”€â”€ 2025-10-20-002.md
â”œâ”€â”€ weekly-reviews/
â”‚   â””â”€â”€ 2025-W42.md             # Weekly summary
â””â”€â”€ insights/
    â””â”€â”€ patterns.md              # Detected patterns
```

### Automatic Analysis

**Pattern Detection**:
- Repeated friction points â†’ Priority bugs
- High usage tools â†’ Preserve and enhance
- Low usage tools â†’ Investigate why or deprecate
- Performance trends â†’ Proactive optimization
- Feature requests clustering â†’ Roadmap validation

**Smart Prioritization**:
```
Priority Score = (Impact Ã— Frequency Ã— User Feedback) / Effort

High Priority:
- Frequent friction with high impact
- Performance regressions
- Bugs affecting core workflows

Medium Priority:
- Nice-to-have improvements
- Low-frequency issues
- Feature requests

Low Priority:
- Edge cases
- Low usage features
- Future phase ideas
```

---

## Implementation Timeline

### Epic 5: Diagnostics & Best Practices
**Story 5.6**: Basic metrics collection
- Tool invocation logging
- Performance measurement
- Error tracking

### Epic 6: Memory & Configuration
**Story 6.6**: Feedback system
- `log_feedback` MCP tool
- Natural language feedback parsing
- Quick reaction system

**Story 6.7**: Analytics & insights
- Weekly summary generation
- Pattern detection
- Priority scoring

---

## Usage Guide

### For You (Josh)

**Daily**:
- Just use Contextualizer naturally
- When something feels good/bad, say it: "Log feedback: X was great/annoying"
- React to quick prompts (y/n/skip)

**Weekly** (5 minutes):
- Review auto-generated summary
- Answer 3 optional questions
- That's it!

**Monthly** (optional):
- Review feedback trends
- Validate roadmap priorities

### What Gets Automated

**Zero Effort From You**:
- All metrics collection
- Performance tracking
- Usage pattern detection
- Issue creation for bugs
- Backlog management
- Priority scoring
- Weekly summary generation

**Minimal Effort From You**:
- Natural language feedback (just talk)
- Quick reactions (one character)
- Weekly questions (optional, 3 quick questions)

---

## Success Metrics

**System is Working When**:
1. You naturally share feedback without thinking about it
2. Bugs are caught and logged before you forget them
3. Ideas are captured in the moment
4. Metrics inform roadmap decisions
5. Friction points are identified and resolved quickly
6. Positive patterns are preserved and enhanced

**Evidence**:
- Regular feedback submissions (weekly+)
- High action rate on collected feedback
- Roadmap alignment with actual usage
- Decreasing friction over time
- Increasing satisfaction scores

---

## Example Workflow

### Scenario: Using init_project

```
You: "Set up this project for Next.js development"

Claude: [invokes init_project]
        âœ… Project initialized with web-fullstack preset
        Created: .claude/hooks/, CLAUDE.md, config.yaml
        Setup time: 2.8s

        [Auto-logged: init_project, 2.8s, web-fullstack]

        Quick feedback: Helpful? (y/n/skip)

You: "y but setup time felt slow"

Claude: âœ… Logged positive feedback
        ğŸ“ Noted: Performance concern (2.8s)

        [Auto-created investigation task if pattern detected]
```

### Scenario: Context Warning

```
Claude: "âš ï¸ Context at 82%. Consider clearing if starting new task."
        [Auto-logged: warning triggered at 82%]

You: [clears context]

        [Auto-logged: warning â†’ action in 30s, effective=true]

        // Later, if you ignore multiple warnings:
        [Auto-logged: warning â†’ ignored, ineffective=true]
        [System learns: maybe thresholds need adjustment]
```

---

**Remember**: The best feedback system is one you don't notice. It should capture insights effortlessly while you focus on development.

---

**Last Updated**: 2025-10-19
**Implementation**: Epic 5-6
**Status**: Planning complete, ready for implementation
